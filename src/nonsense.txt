
    #k_means_func = None
    
    #base_list = []
    #for i in len(song_data):
    #    base_list.append(np.full(vd.input_count/2, 2))
    #data_spliced = np.asarray(base_list)#np.full((len(song_data), (vd.input_count/2, 2)), 0)
    #total_labels = np.full((len(song_data)), 0)
    
    # Splice song data into 2 dimensional vectors for individual training method
    '''
    for index, song in enumerate(song_data):
        for i in range(vd.input_count):
            data_spliced[index][int(i/2)][i%2] = song[i]
    
    first_layer = np.full(len(song_data), 2)
    for index, song in enumerate(data_spliced):
        first_layer[index] = song[index][0]
        
    k_means_func = KMeans(n_clusters=k, n_init=25).fit(first_layer)
    
    for i in range((vd.input_count/2)-1):
        next_layer = np.full(len(song_data), 2)
        for index, song in enumerate(data_spliced):
            next_layer[index] = song[index][i+1]
        labels = k_means_func.predict(next_layer)
        total_labels = total_labels + labels
        
    final_labels = total_labels / (vd.input_count/2)
    '''
    
    #categories = {}
    #for i in range(k):
    #    categories[("" + str(i))] = []
    
    #for index, song_title in enumerate(vd.array_audio[0]):
    #    categories[("" + str(k_means_func.labels_[index]))].append(song_title)
    
    #for i in range(k):
    #    print("\n\nCategory", i, ":", categories["" + str(i)])
        
        # Data for plotting
    t = np.arange(0.0, 2.0, 0.01)
    s = 1 + np.sin(2 * np.pi * t)

    fig, ax = plt.subplots()
    ax.plot(t, s)

    ax.set(xlabel='time (s)', ylabel='voltage (mV)', title='About as simple as it gets, folks')
    ax.grid()

    #fig.savefig("test.png")
    #plt.show()
    
    #song_data2 = np.full((100, 100), 40)
    #song_data2[50][50] = 1
    #rand_data = np.full((2, 100), 20)
    
    
    #print("Shape : ", song_data.shape)
    
    #print("\nNumber of feats:", num_feats, ", Number of points:", num_pts)
    
    #k = 10
    #k_means_func = KMeans(n_clusters=k, n_init=5).fit(song_data2)
    #print("\nLabels: ", k_means_func.labels_)
    #print("\nLabels: ", k_means_func.cluster_centers_)
    #print("\nGuess : ", k_means_func.predict(rand_data))
    
    '''
    reduced_data = PCA(n_components=2).fit_transform(song_data)
    kmeans = KMeans(init="k-means++", n_clusters=k, n_init=4)
    kmeans.fit(reduced_data)

    # Step size of the mesh. Decrease to increase the quality of the VQ.
    h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].

    # Plot the decision boundary. For that, we will assign a color to each
    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1
    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1
    exit()
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # Kills program?

    # Obtain labels for each point in mesh. Use last trained model.
    Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])

    # Put the result into a color plot
    Z = Z.reshape(xx.shape)
    plt.figure(1)
    plt.clf()
    plt.imshow(
        Z,
        interpolation="nearest",
        extent=(xx.min(), xx.max(), yy.min(), yy.max()),
        cmap=plt.cm.Paired,
        aspect="auto",
        origin="lower",
    )

    plt.plot(reduced_data[:, 0], reduced_data[:, 1], "k.", markersize=2)
    # Plot the centroids as a white X
    centroids = kmeans.cluster_centers_
    plt.scatter(
        centroids[:, 0],
        centroids[:, 1],
        marker="x",
        s=169,
        linewidths=3,
        color="w",
        zorder=10,
    )
    plt.title(
        "K-means clustering on the digits dataset (PCA-reduced data)\n"
        "Centroids are marked with white cross"
    )
    plt.xlim(x_min, x_max)
    plt.ylim(y_min, y_max)
    plt.xticks(())
    plt.yticks(())
    plt.show()
    '''

def bench_k_means(kmeans, name, data, labels):
    """Benchmark to evaluate the KMeans initialization methods.

    Parameters
    ----------
    kmeans : KMeans instance
        A :class:`~sklearn.cluster.KMeans` instance with the initialization
        already set.
    name : str
        Name given to the strategy. It will be used to show the results in a
        table.
    data : ndarray of shape (n_samples, n_features)
        The data to cluster.
    labels : ndarray of shape (n_samples,)
        The labels used to compute the clustering metrics which requires some
        supervision.
    """
    t0 = time()
    estimator = make_pipeline(StandardScaler(), kmeans).fit(data)
    fit_time = time() - t0
    results = [name, fit_time, estimator[-1].inertia_]

    # Define the metrics which require only the true labels and estimator
    # labels
    clustering_metrics = [
        metrics.homogeneity_score,
        metrics.completeness_score,
        metrics.v_measure_score,
        metrics.adjusted_rand_score,
        metrics.adjusted_mutual_info_score,
    ]
    results += [m(labels, estimator[-1].labels_) for m in clustering_metrics]

    # The silhouette score requires the full dataset
    results += [
        metrics.silhouette_score(
            data,
            estimator[-1].labels_,
            metric="euclidean",
            sample_size=300,
        )
    ]

    # Show the results
    formatter_result = (
        "{:9s}\t{:.3f}s\t{:.0f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}\t{:.3f}"
    )
    print(formatter_result.format(*results))

'''
def data_group_avg(group_ids, data):
    # Sum each group
    sum_total = tf.compat.v1.unsorted_segment_sum(data, group_ids, 3)
    # Count each group
    num_total = tf.compat.v1.unsorted_segment_sum(tf.ones_like(data), group_ids, 3)
    # Calculate average
    avg_by_group = sum_total/num_total
    return(avg_by_group)

def train_music_model():
    vd.convert_list_to_array()
    sess = tf.compat.v1.Session()

    song_data = vd.array_audio[1]

    num_pts = len(song_data)
    num_feats = vd.input_count
    k=3 # implement elbow method later to get k automatically
    generations = 25
    data_points = tf.compat.v1.Variable(song_data)
    cluster_labels = tf.compat.v1.Variable(tf.zeros([num_pts], dtype=tf.int64))

    rand_starts = np.array([song_data[np.random.choice(len(song_data))] for _ in range(k)])
    centroids = tf.compat.v1.Variable(rand_starts)

    centroid_matrix = tf.compat.v1.reshape(tf.compat.v1.tile(centroids, [num_pts, 1]), [num_pts, k, num_feats])
    point_matrix = tf.compat.v1.reshape(tf.compat.v1.tile(data_points, [1, k]), [num_pts, k, num_feats])
    distances = tf.compat.v1.reduce_sum(tf.compat.v1.square(point_matrix - centroid_matrix), reduction_indices=2)

    centroid_group = tf.compat.v1.argmin(distances, 1)

    means = data_group_avg(centroid_group, data_points)
    update = tf.compat.v1.group(centroids.assign(means), cluster_labels.assign(centroid_group))

    init = tf.compat.v1.initialize_all_variables()
    sess.run(init)

    for i in range(generations):
        print('Calculating gen {}, out of {}.'.format(i, generations))
        _, centroid_group_count = sess.run([update, centroid_group])
        group_count = []
        for ix in range(k):
            group_count.append(np.sum(centroid_group_count==ix))
        print('Group counts: {}'.format(group_count))
'''